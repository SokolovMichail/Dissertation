\documentclass[14pt]{mmcs_article}
\usepackage[russian]{babel}
\usepackage{amsmath, amsthm, amsfonts, amssymb}
\usepackage{tikz}
\usetikzlibrary{positioning}



\newenvironment{myenumerate}
{ \begin{enumerate}
		\setlength{\itemsep}{0pt}
		\setlength{\parskip}{0pt}
		\setlength{\parsep}{0pt}     }
	{ \end{enumerate}                  } 

%\graphicspath{{images/}}%путь к рисункам

\begin{document}

% Титульные листы
% раскомментировать требуемое
%\include{Titul3k} % для курсовой
%\include{TitulBak}% для работы бакалавра
\include{TitulMag}% для работы магистра

\renewcommand{\contentsname}{Оглавление}

\tableofcontents

%=======================
\newpage
\addcontentsline{toc}{section}{Постановка задачи}

\section*{Постановка задачи}


Цель работы - создание мета-рекомендательной системы, способной адаптироваться и давать рекомендации для любого интернет-магазина. Для достижения цели был сформирован набор рабочих задач:
\begin{enumerate}
	\item Анализ полученных из интернет-магазинов треков активности пользователей и описания продаваемых товаров
	\item Выбор методов рекомендации, подбор типов моделей-рекомендаторов.
	\item Формирование представлений данных для создания и обучения моделей.
	\item Обучение моделей и поиск оптимальных гиперпараметров.
	\item Добавление возможности автоматического пересоздания представлений данных и моделей и переобучения моделей в связи с изменившимися данными и возможности использования оперативной истории без переобучения моделей
	\item Интеграция в существующую платформу интернет-магазинов.
\end{enumerate}

Для решения задачи( по согласованию с заказчиком) был выбран ЯП Python с пакетами numpy v 1.19.5, scikit-learn v 1.0.0, scipy v 1.7.1, implicit v 0.4.4, pandas v 1.3.5 и TensorFlow v 2.4.2, а также использовался NVidia CUDA Toolkit v 10.1 для ускорения обучения определенных нейросетевых моделей с использованием GPU.

В дальнейшем может быть апдейтнем версию TensorFlow до 2.7.*... Но пока нет. 


%=======================
\newpage
\addcontentsline{toc}{section}{Введение}
\section*{Введение}

Дальнейшее повышение эффективности работы субьектов электронной коммерции, на наш взгляд, связано с расширением списков клиентов, получающих доступ к упомянутым субьектам, а также с персонализацией онлайн-маркетинга. По оценкам McKinsey \cite{INTRO:a1}, 35\% выручки Amazon или 75\% Netflix приходится именно на рекомендованные товары и процент этот, вероятно, будет расти. В данном отчете описана попытка создания решения на базе собранной информации интернет-магазинов. 

Задача рекомендательной системы – проинформировать пользователя о товаре, который ему может быть наиболее интересен в данный момент времени. Клиент получает информацию, а сервис зарабатывает на предоставлении качественных услуг. Услуги — это не обязательно прямые продажи предлагаемого товара. Сервис также может зарабатывать на комиссионных или просто увеличивать лояльность пользователей, которая потом выливается в рекламные и иные доходы.

В зависимости от модели бизнеса рекомендации могут быть его основой, как, например, у компании TripAdvisor, а могут быть просто удобным дополнительным сервисом (как, например, в каком-нибудь интернет-магазине одежды), призванным улучшить Customer Experience и сделать навигацию по каталогу более удобной. В данной работе рекомендательная система используется как дополнительный сервис в дополнение к основному сервису(интернет-магазин), в качестве предмета рекомендаций используются товары магазина. 

Эта работа может быть использована в предприятиях электронной коммерции (интернет-магазинах) для быстрого и легкого создания и обновления автоматической рекомендательной системы и соответственно, имеет высокую практическую значимость.
Результаты данной работы апробированы на конференции "Математика. Компьютер. Образование. 2022".

%=======================
\newpage
% вставить ссылку на одну из ALS статей Collaborative Filtering for Implicit Feedback Datasets
\section{Explicit Feedback и Implicit Feedback}
Рекомендательные системы используют два основных типа обратной связи от пользователей. Это explicit feedback(явная обратная связь) и implicit feedback(неявная обратная связь). Приведем примеры каждой из обратных связей.

Явная обратная связь - это оценки контента пользователями. Такими оценками могут быть, например, количество звезд, числовые оценки по некоторым шкалам, и даже простое нравится/не нравится, использующееся, допустим, видеохостингом YouTube.

Неявная обратная связь - это любые записанные действия пользователя при взаимодействии с контентом. Просмотры товаров, история покупок, добавление товара в корзину, покупка товара - все это является одним из вариантов implicit feedback. 

Эти два типа связей значительно отличаются друг от друга по большому количеству пунктов.
\begin{myenumerate}
\item Количество данных обратной связи. Для явной обратной связи количество собранных данных всегда значительно меньше, чем количество данных для неявной обратной связи. Кроме того, не всегда в системах вообще существуют встроенные механизмы  сбора данных явной обратной связи и/или возможность их создания. 
\item Негативные оценки. При явной обратной связи всегда существует явная негативная обратная связь(или возможность ее установления). Так, допустим, недовольный товаром человек может поставить в оценке одну звезду из пяти возможных. При неявной обратной связи же практически невозможно установить негативный feedback, любая такая связь сама по себе будет позитивной. Так, допустим, отсутствие записанного взаимодействия пользователя и конкретной части контента может быть как в силу негативного отношения пользователя к данной части контента, так и в силу отсутствия взаимодействия этих пользователей вообще.  
\item Наличие шума в данных. Когда мы проводим сбор implicit feedback, невозможно достоверно установить их предпочтения и истинные мотивы совершаемых ими действий. Так, например, данные о покупке(или просмотре/добавлении в корзину) пользователем продукта далеко не всегда являются следствием хорошего отношения пользователя к продукту. Существует вероятность что продукт был приобретен в качестве подарка, или что пользователь был разочарован при использовании продукта.
\item Отличие значения данных. Так, при явной обратной связи числовое значение(количество покупок, оценка качества) означает предпочтение одному продукту другого, в то время как при неявной обратной связи это означает лишь частоту взаимодействия. Тем не менее, хотя более высокая частота взаимодействия не всегда отражает предпочтение одного продукта другому, повторяющееся действие с более высокой вероятностью показывает предпочтения пользователя.
\item Учет дополнительных признаков. Так, при неявной обратной связи необходимо учитывать также доступность каждого конкретного продукта в момент времени, повторяемые покупки и т.д. При явной обратной связи такое не требуется.
\end{myenumerate}
По причинам, которые будут озвучены ниже, в работе используется только implicit feedback.

\section{Входные данные. EDA}\label{dsfs}
Заказчиком были предоставлены экземпляры файлов, выгружаемых с с интернет-магазинов. 
Файл 1 типа - .csv  файл, содержащий описание событий происходящих в интернет-магазине, в дальнейшем - данные о событиях. \\
Всего файл 1 типа содержит 44 типа полей, однако некоторые из них не заполнены во всем датасете. Далее следует список полей и гипотез для каждого поля. Незаполненные поля, а также поля не несущие полезной информации(сервисные хеши и т.д) опущены.
\begin{myenumerate}
	\item datetime. Поле времени начала события. 
	\item userip. Поле, в котором указывается IPv4-адрес, с которого осуществлялся доступ. Гипотеза: можно использовать для генерации рекомендаций по стране пользователя.
	\item userid. Цифробуквенный уникальный ID пользователя в системе, основной способ идентификации пользователя.
	\item useragent. Фрагмент HTML-header, указывающий на используемый браузер. Гипотеза: можно использовать тип браузера для дифференциации положения владельца в обществе. Нельзя. TODO
	\item eventtype. Тип произошедшего события. 
	\begin{myenumerate}
		\item ProductView - события просмотра товара пользователем. 
		\item AddToCart - события добавления пользователем товара в корзину. 
		\item FacetSelection - событие выбора пользователем фильтра показа товаров.  
		\item Search - событие поиска пользователем товара по названию.
		\item AutoComplete - событие автодополнения запроса пользователя. 
		\item ClickOnSearchResult - событие нажатия на результат поисковой выдачи. 
	\end{myenumerate}
	\item usersearchphrase Запрос пользователя. Используется в типах событий AutoComplete, Search. Гипотеза: Может быть использовано для лингвистического анализа запроса.
	\item correctedsearchphrase. Исправленный запрос пользователя. Используется в типе событий Search. 
	\item facetname. Имя фильтра по которому производился поиск. Используется в типе событий Facet Selection. 
	\item facetvalue. Значение фильтра. Используется в типе событий Facet Selection. Гипотеза: может быть использовано при рекомендации, отфильтровывая рекомендации по соответствующему значению. 
	\item productid. Внутренний уникальный ID продукта. 
\end{myenumerate}

Файл 2 типа - .json файл содержащий описания товаров, а также фильтров товаров, представленных в магазине. Вследствие предполагаемой универсальности системы, а также большого количества полей, являющимися пустыми, приведем описание только используемых универсальных полей, и обобщенное описание остальных полей.\\
В данном файле интерес представляют два поля: Items и Facets. Рассмотрим их подробнее. \\
Поле Items:
\begin{myenumerate}
	
	\item CatalogID. Уникальный числовой ID продукта. Совпадает со значением поля productid в событиях типа ProductView.
	\item Name. Имя товара. Гипотеза: может быть использовано для поиска похожих по названию товаров.
 	\item OrdersCount. Количество заказанных товаров. Гипотеза: можно использовать для фильтрации товаров по популярности.
	\item Типы и значения фильтров, отвечающих товару.
\end{myenumerate}

Поле Facets:
\begin{myenumerate}
	
	\item FacetName. Имя фильтра.
	\item TotalHits. Количество раз, который этот фильтр был выбран. Гипотеза: Может быть использовано для определения самых часто выбираемых фильтров.
	\item Values. Список возможных значений, которые может принимать фильтр. Может иметь как категориальный тип, так и числовой тип. 
\end{myenumerate}

Особо следует отметить, что эти стандартизированные выгрузки данных не содержат в себе ни информации об оценках пользователями купленных товаров, ни информации о покупках пользователя, то есть в наличии только implicit feedback.

После проведенного EDA, было принято решения использовать лишь следующие поля:

Из файлов 1 типа(событий):

\begin{myenumerate}
		\item datetime.
		\item userid. 
		\item productid.
		\item eventtype. Было принято решение для простоты использовать исключительно типы событий ProductView и AddToCart, так как они несут наибольшее количество полезной информации для рекомендательной системы. 
\end{myenumerate}

Из файлов 2 типа(предметов):

\begin{myenumerate}
	\item CatalogID.
	\item Типы и значения фильтров, отвечающих товару
\end{myenumerate}

Кроме того, такой подход позволяет значительно сократить количество используемой ОЗУ и памяти для хранения данных.

Для обработки данных был создан специализированный пайплайн. Он состоит из 6 стадий обработки данных, которые можно представить в виде следующего графа.

\begin{figure}[H]
\begin{tikzpicture}[main/.style = {draw, rectangle},node distance= 1cm,align=left] 
\node[main] (1) {prepare\_actions(1)}; 
\node[main] (2) [below=0.5cm of 1] {prepare\_df\_splits(2)};
\node[main] (3) [below left=0.5cm and -1cm of 2] {prepare\_csr\_splits(3)}; 
\node[main] (4) [below right=0.5cm and -2cm of 2] {prepare\_items\_by\_users(4)}; 
\node[main] (5) [right=1cm of 1] {prepare\_item\_profiles(5)}; 
\node[main] (6) [below right=0.5cm and -2cm of 4] {prepare\_user\_profiles(6)}; 
\draw[->] (1) -- (2);
\draw[->] (2) -- (3);
\draw[->] (2) -- (4);
\draw[->] (4) -- (6);
\draw[->] (5) -- (6);
\end{tikzpicture}
\caption{Граф пайплайна системы}\label{stud:fig:2}
\end{figure}
На каждом этапе формируется одно из представлений данных. Кратко опишем их.

\begin{myenumerate}
	\item Стадия prepare\_actions. На ней формируется временное представление - $pandas$.$dataframe$, содержаший в себе записи о всех событиях типа  ProductView и AddToCart.
	\item Стадия prepare\_df\_splits. На ней формируется основное представление данных, представляющее собой два  $pandas$.$dataframe$ - соответственно train и test. train содержит в себе все события, произошедшие до определенной даты включительно, test - все события, которые произошли после определенной даты. Важно, что test содержит в себе события только тех пользователей, которые входят во множество train. 
	\item Стадия prepare\_csr\_splits. На ней, на основе результатов стадии \\ prepare\_df\_splits формируются два основных представления данных CSR(csr\_train и csr\_test) - разреженные матрицы формата \\$scipy$.$sparse$.$csr\_matrix$, содержащие все взаимодействия пользователей со всеми товарами(c повышающими коэффициентами для событий AddToCart). Так, одно событие AddToCart рассматривается как одно или более событий ProductView - конкретный коэффициент задается в конфигурационном файле системы. Путем экспериментов, был подобран коэффициент = 10.
	\item Стадия prepare\_items\_by\_users. На этой стадии создается словарь взаимодействия userid и productid, что используется для контроля обучения моделей и в стадии prepare\_user\_profiles.
	\item Стадия prepare\_item\_profiles. На этой стадии формируется представление данных ItemProfileStorage - создается таблица(pandas.DataFrame), содержащая профили всех товаров, построенная на извлеченных из .json файла фильтров показа и их значений. Строки таблицы - товары, столбцы - характеристики товаров. 
	\item Стадия prepare\_user\_profiles. На этой стадии формируется представление данных UserProfileStorage - таблица(pandas.DataFrame), содержащая профили всех пользователей, построенная на извлеченных из .json файла фильтров показа и их значений и представлении данных IP. Каждый профиль пользователя представляет собой алгебраическую сумму профилей всех товаров, с которыми он взаимодействовал(точно так же, как в CSR - c повышающими коэффициентами для событий AddToCart). Это основа для собственного подхода, объединяющего принципы Content based подхода и коллаборативной фильтрации.
	
\end{myenumerate}

Основные представления данных, которые будут использованы в дальнейшем - CSR, ItemProfileStorage, UserProfileStorage.


%=======================

\section{Модели}\label{dsfs}
\subsection{Исследование трудов по теме}
Как уже говорилось выше, поставленная задача - создание максимально универсальной рекомендательной системы, подходящей для любого интернет-магазина при условии стандартизированной выгрузки данных.
Предварительно стоит отметить, что есть две категории товаров:
\begin{itemize}
	\item Повторяемые товары. Это те товары-расходники, которые люди покупают часто. К таким относятся, например, продукты питания, бритвенные станки, предметы бытовой химии.
	\item  Неповторяемые товары. Это такие товары, которые редко приобретают повторно. Примеры таких товаров - электроника, бытовая техника, ювелирные украшения.
\end{itemize}
Давайте опишем нашу рекомендательную систему:
\begin{enumerate}
\item Предмет рекомендации. Для данного проекта предметом рекомендации может являться только товар. Ситуация с применением системы для магазинов, использующих услуги не рассматривалась. В силу предполагаемой максимальной универсальности системы, она предполагает одинаковое отношение как к товарам из повторяемой группы, так и к товарам из неповторяемой группы.
\item Цель рекомендации. Здесь цель рекомендации - информирование пользователя о товарах, которые могут ему подойти, и в конечном счете - покупка пользователем дополнительных товаров.
\item Контекст рекомендации. На момент получения рекомендации предполагается, что пользователь смотрит товары и/или находится в корзине.
\item Иcточники рекомендации. Здесь это как общая аудитория магазина, так и схожие по интересам пользователи, в зависимости от подхода.
\item Степень персонализации рекомендации. Здесь в зависимости от подхода используются разные степени персонализации.
\item Алгоритм рекомендации. Об этом будет написано ниже. 
\end{enumerate}

Путем исследования, было установлено, что существует три основных подхода к созданию моделей для решения следующих задач - Summary-based модели, Content-based модели и Collaborative filtering модели. В подходе collaborative filtering также выделяют отдельный субподход - matrix factorization.  Опишем отличительные особенности этих подходов. 

Подход Summary-based. Данный подход является наиболее простым - и, тем не менее, достаточно эффективным. Он основан на неперсонализированной оценке популярности каждого товара, и соответственно, рекомендации пользователю самых популярных товаров.

Подход Collaborative Filtering. Данный класс систем начал активно развиваться в 90-е годы. В рамках подхода рекомендации генерируются на основании интересов других похожих пользователей, таким образом являясь результатом «коллаборации» множества пользователей. Отсюда и  происходит название метода. Этот метод уже является персонализированным - т.е. рекомендации подбираются персонально под каждого пользователя.

Одними из главных субподходов для collaborative filtering является подходы на основе факторизации матриц - так называемые "matrix factorization". В основе такого субподхода лежит матрица товар-клиент - разреженная матрица взаимодействия оценок товаров и пользователей. Каждое значение суть мера заинтересованности пользователя в товаре. 

Подход  Content-based. Данный подход основан на описании товара. Для товаров создаются признаковые описания, представляющие собой векторы категориальных признаков.
В рамках подхода рекомендуются товары, которые наиболее похожи на популярные у пользователя продукты.

\subsection{Проблема холодного старта}
Проблема холодного старта - одна из типичных ситуаций для рекомендательной системы. Заключается она в том, что в момент добавления нового пользователя и товара о нем практически ничего не известно. В силу универсальности создаваемой системы, принято решение данную проблему игнорировать. Так, предположительно, система будет пересоздаваться и переобучаться каждые 168 часов, что должно нивелировать отсутствие механизмов смягчения проблемы холодного старта. Кроме того, модели item-based не подвержены этой проблеме.

\subsection{Общее описание моделей}
Далее были созданы одна модель - baseline и 6 моделей машинного обучения. Перечислим их по порядку: TopN, ALS, BPR, AE, IP, UIP, DRN. Выход каждой модели - список товаров, которые необходимо рекомендовать пользователю. \\
Стоит заметить, что процесс генерации и обучения моделей также является полностью автоматизированным, что позволяет переобучать модели без участия ml-инженеров. К сожалению, все модели(за исключением Top30) жестко привязываются к данным, что делает процесс дообучения на новых данных невозможным, вследствие чего используется процесс полного переобучения.
\subsection{TopN}
Самая очевидная и простая модель - любому пользователю предлагается N самых популярных товаров. Данная модель используется в качестве baseline для всех остальных моделей, а также для дополнения списка рекомендаций в случае если другие модели не смогли дать необходимое их число.
\subsection{ALS}
Модель ALS \cite{ALSA1}. Данная модель принадлежит к классу collaborative filtering. \\
Введем отношения, необходимые для работы данной модели.

Примем общее количество пользователей на данный момент времени в системе как $m$, общее количество товаров на данный момент времени в системе как $n$.Запишем все взаимодействия пользователей и продуктов в матрицу $R_{m \times n}$, где $R_{i,j}$ - количество раз которое пользователь $i$ взаимодействовал(в нашем случае просматривал) товар $j$.  Результирующая матрица является разреженной, так как на практике практически невозможна ситуация когда каждый пользователь взаимодействует с каждым товаром, что отражается нулевыми значениями в матрице.

Статья \cite{ALS:CFIFD} предлагает ввести переменные $p_{ui}$, которые можно принять как меру предпочтения пользователя $u$ товару $j$. Для удобства использования, бинаризуем их. Тогда
\begin{equation}
	p_{ui} = \begin{cases}
		1 & r_{ui} > 0 \\
		0 & r_{ui} = 0
	\end{cases}
\end{equation}

Тем не менее, сразу использовать эти значения $p_{ui}$ нерационально. Статья \cite{ALS:CFIFD} предлагает введение дополнительных переменных $c_{ui}$ таких, что $c_{ui} = 1 + \alpha * r_{ui}$. Эти переменные отвечают за меру нашей уверенности в значении $p_{ui}$ Тогда всегда будет существовать ненулевое предпочтение пользователя $u$ товару $i$, и при этом чем больше количество взаимодействий пользователя и товара зафиксировано, тем более высокое значение примет $c_{ui}$. $\alpha$ здесь - повышающий коэффициент, который предлагается принять равным 40. 

Рассмотрим задачи и процесс работы алгоритма. 

Задача алгоритма - найти такие векторы $x_u \in \mathbb {R}^f$ для каждого пользователя u, $y_i \in \mathbb {R}^f$ для каждого продукта $i$ которые бы отвечали предпочтениям пользователя. Отсюда, можно составить следующее равенство: $p_{ui} = x_u^Ty_i$. Заметим, что этот процесс похож на процесс матричного разложения, часто использующийся в рамках рекомендаций для явной обратной связи, однако имеет и два важных отличия:
\begin{enumerate}
	\item Необходимо учитывать то, что значения $c_{ui}$ различны.
	\item Необходимо, чтобы оптимизация учитывала все возможные пары $u$, $i$, а не только те, что уже известны.
\end{enumerate}
Соответственно, разложения вычисляются путем минимизации значения следующей функции:
\begin{equation}\label{eq:1}
	\sum_{x*,y*} c_{ui} (p_{ui} - x_u^Ty_i)^2 + \lambda(\sum_u \parallel x_u \parallel ^2 + \sum_i \parallel y_i \parallel ^2) -> min
\end{equation}
Здесь выражение $\lambda(\sum_u \parallel x_u \parallel ^2 + \sum_i \parallel y_i \parallel ^2)$ используется для регуляризации модели, что позволяет избежать переобучения.

Очевидно, что для данной задачи значения $m$ и $n$ могут быть достаточно большими, и расчет "в лоб" неприемлем. Но можно заметить, что при условии фиксации векторов товаров или пользователей, сложность расчета функции резко падает до квадратичной. Соответственно, процесс работы алгоритма заключается в попеременной фиксации векторов пользователей и товаров и перерасчете незафиксированного вектора. Рассмотрим его подробнее.

Шаг 1 - перерасчет векторов пользователей. Пусть все векторы товаров соединены в единую матрицу, чья размерность будет $ n \times f $. Прежде чем осуществлять перерасчет всех векторов пользователей, найдем за время $O(f^2n)$ матрицу $Y^TY$ размерности $f \times f$. Для каждого пользователя введем диагональную матрицу $C^u$ : $C^u_{ii} = c_ui$ и вектор $p_(u) \ in \mathbb {R}^f$, который содержит $p_{ui}$. Отсюда можно найти выражение для $x_u$:
\begin{equation}
	x_u = (Y^TC^uY + \lambda I)^-1Y^TC^up_u
\end{equation}

Шаг 2

%OLD_VARIANT_ALS 
%ALS(Alternating Least Squares) это итеративный процесс факторизации матрицы $R$ на матрицы $U_{m \times k}$ и $V{k %\times n}$ такие, что  $R \approx U^TV$. Здесь $k$ означает количество скрытых признаков.\\
%Для нахождения матриц $U$, $V$ необходимо решить следующую оптимизационную задачу: 
%\begin{equation}
%argmin_{U,V} \sum_{(i,j|r_{i,j} != 0)} (r_{i,j} - U_i^TV_j)^2 + \lambda (\sum_i n_{u_i} {\parallel u_i \parallel} ^2 %+ %\sum_j n_{v_j} {\parallel v_j \parallel} ^2) 
%\end{equation}
%Здесь $\lambda$ - сила регуляризации, $n_{u_i}$ - количество продуктов просмотренных пользователем $i$,  $n_{v_j}$ - %количество раз, которые был просмотрен товар $j$. \\
%Поочередно фиксируя матрицы $U$ и $V$, мы получаем квадратное уравнение, которое можно решить, и , таким образом, %итеративно улучшить качество разложения. \\
%Очевидно, что данная модель использует представление данных CSR. В качестве реализации данной модели была взята %библиотека implicit \cite{ALSA2} для ЯП Python.


%\subsection{BPR}
%Модель BPR также принадлежит к классу collaborative filtering. \\
%Для работы модели необходимы ровно те же отношения, что и для работы модели ALS. Внутри, впрочем данная %модель использует метод попарного ранжирования отношения пользователя $i$ к продуктам $j1$ и $j2$. 
%Данная модель также использует модель представления CSR.
\subsection{AE}
Модель AE(AutoEncoder) принадлежит к классу collaborative filtering. \\

Для работы модели необходима матрица взаимодействия пользователь-товар. Идея данного подхода заключается в том, чтобы С помощью нейронной сети предпринимается попытка восстановить строку матрицы взаимодействия пользователь-товар.\cite{AEA1} \\

Как известно, автоенкодер представляет собой нейронную сеть, выполняющую 2 преобразования  $encode(x) : Rn \rightarrow Rd$ и $decode(x) : Rd \rightarrow Rn$. Цель этих преобразований получить представление данных исходной размерности $n$ в размерности $d$ такую, чтобы минимизировать отклонение $x\_received =decode(encode(x))$.

Данная модель представляет собой достаточно простой автоенкодер. Енкодер представлен одним Dense Layer с размерностью входа(n) равной количеству товаров, и размерностью выхода(Bottleneck) экспериментально подобранной как 256(d). Декодер, соответственно, представлен одним DenseLayer с размерностью входа($d$) 256 и размерностью выхода ($n$) равной количеству товаров. 

Особо стоит отметить процесс выбора функций активации и loss-функции. Вследствие характера обрабатываемых данных(implicit feedback пользователей), нежелательна потеря отрицательных значений после активации, что делает нерациональным использование стандартного в таких случаях ReLU( Rectified Linear Unit). Поэтому в качестве функции используется ELU(Exponential Linear Unit). В дальнейшем будет произведен эксперимент с использованием SELU(Scaled Exponential Linear Unit).\\

В подобных задачах часто используют masked mse (loss оценивается только по не нулевым позициям входного вектора, позволяя сети сколько угодно сильно "ошибаться" по тем позциям, где стояли нули.) в качестве loss-функции.  Но при решении данной задачи этот метод не сработал. Обычный mse отлично справляется и не может занулить все наши рекомендации просто по тому, то автоэнкодер не может дать 100\% точность после разжатия данных.


\subsection{IP}
Модель IP является представителем item-based подхода. \\
Построенные в представлении данных IP профили товаров используются как обучающий dataset для модели sklearn.NearestNeighbours, имеющий в основе алгоритм KD-Tree. Данная модель позволяет как рекомендовать товары, похожие на данный, так и рекомендовать товары на основе истории пользователя.  
В случае рекомендаций товаров на основе истории взаимодействий пользователя, применяется следующий алгоритм:

Правило брать не более чем 3 похожих на текущий товара выведено экспериментально.
\subsection{UIP}
Аналогично, построенные в представлении данных UIP профили пользователей используются как обучающий dataset для модели sklearn.NearestNeighbours, имеющий в основе алгоритм KD-Tree. Данная модель позволяет находить пользователей, похожих на данного, и, соответственно, рекомендовать пользователю товары, являющиеся популярными у похожих пользователей.
Применяется следующий алгоритм:

Аналогично, правило брать не более чем 5 товаров у похожего пользователя выведено экспериментально.
\subsection{DRN}

Данная модель примечательна тем, что она является гибридной, совмещая в себе подходы collaborative filtering и content-based. Также, она примечательна и своей конструкцией - это сиамская нейронная сеть, имеющая структуру, приведенную ниже. Задача этой сети - обучиться отличать вещи которые могут понравиться пользователю. Необычен и процесс обучения - на вход данной модели подаются тройки вида (профиль пользователя, профиль понравившейся ему вещи, профиль не понравившейся ему вещи).

Данная модель использует две подмодели - для создания embedding товаров (Dense Layer размерности 64, ReLU, Dense Layer размерности 32, ReLU) и embedding пользователей (Dense Layer размерности 32, ReLU). Оба embedding передаются в ScoreLayer, где определяется близость embedding пользователя к embedding не понравившейся ему вещи и близость embedding пользователя к embedding понравившейся ему вещи.  Финальный слой -TripletLossLayer - определяет разницу score.

Стоит отметить, что слои TripletLossLayer и ScoreLayer реализованы специально для этой модели.

Из-за сложности и специализированных слоев данная модель имеет низкую скорость обучения, и не показывает высоких результатов. Вследствие этого, будет производится исследование возможности доработки данной модели.

\subsection{Сравнительный анализ моделей}
Для сравнительного анализа моделей была рассчитаны метрики для всех моделей на тестовом датасете, содержащем данные за 2 недели, и сведены в единую таблицу.\\
\begin{tabular}{| l |l| l| l| l|}
	\hline
	Модель & DCG@30 & HappyUsersRatio@5 & IOU@30 &  NAP@30 \\
	\hline
	Top30 & 0.02464 & 0.11504 & 0.02029 & 0.00535 \\
	\hline
	ALS & 0.02949 & 0.10442 & 0.01994 &  0.0861 \\
	\hline
	AE & 0.07552 & 0.25841 & 0.04696 &  0.02894 \\
	\hline
	IP & 0.03612 & 0.12249 & 0.02375 &  0.01038 \\
	\hline
	UIP & 0.5002 & 0.26707 & 0.0207 &  0.02035 \\
	\hline
	DRN & 0.00861 & 0.03363 & 0.0027 & 0.00401 \\
	\hline
	Ансамбль & 0.8889 & 0.26707 & 0.04164 &  0.02504 \\
	\hline
\end{tabular}\\ \\
Основополагающей метрикой здесь является метрика HappyUsersRatio.
Как мы видим, наилучший результат показала модель UIP.

%=======================

\section{Создание ансамбля моделей}
Для улучшения результатов, был создан автоматический ансамбль, который выбирает лучшую collaborative-filtering модель и лучшую content-based - модель(в данном случае единственную - IP). Рекомендации от обоих моделей формируются в шахматном порядке, повторы исключаются, а товары, которые предсказали обе модели идут в начало. Вследствие того, что в ансамбле присутствуют модели, сходные по характеристикам, а в результате обучения метрики немного меняются в зависимости от данных, этот подход позволяет потенциально всегда выбирать наилучшие модели для заданных магазинов.
Сравним метрики, полученные таким способом, с уже имеющимися:\\
\begin{tabular}{| l |l| l| l| l|}
	\hline
	Модель & DCG@30 & HappyUsersRatio@5 & IOU@30 &  NAP@30 \\
	\hline
	Top30 & 0.02464 & 0.11504 & 0.02029 & 0.00535 \\
	\hline
	ALS & 0.02949 & 0.10442 & 0.01994 &  0.0861 \\
	\hline
	AE & 0.07552 & 0.25841 & 0.04696 &  0.02894 \\
	\hline
	IP & 0.03612 & 0.12249 & 0.02375 &  0.01038 \\
	\hline
	UIP & 0.5002 & 0.26707 & 0.0207 &  0.02035 \\
	\hline
	DRN & 0.00861 & 0.03363 & 0.0027 & 0.00401 \\
	\hline
	Ансамбль & 0.8889 & 0.26707 & 0.04164 &  0.02504 \\
	\hline
\end{tabular}\\
\\
Следует отметить, что модели AE и UIP имеет одни из лучших метрик, а итоговый ансамбль опережает даже их, что говорит о целесообразности данного подхода.

%=======================

%=======================
\newpage
\addcontentsline{toc}{section}{Заключение}
\section*{Заключение}

Как итог, на данный момент создана универсальная рекомендательная система с адаптивным ансамблем моделей, способная к полностью автоматическому переформированию представлений данных, пересозданию и переобучению моделей, способная работать для любого интернет-магазина при условии предоставления им стандартизированной выгрузки данных. На данный момент производится А/B тестирование на серверах интернет-магазинов для определения действительной эффективности системы. \\
Кроме того, текущий результат работы был опубликован на конференции МКО-2022 в виде тезисов и видеопрезентации доклада. [Приложения 1,2]

Предложения по доработке.
\begin{enumerate}
	\item Добавить возможность разделения часто и редко покупаемых товаров.
\end{enumerate}



%=======================
\newpage

\addcontentsline{toc}{section}{Литература}
\renewcommand{\refname}{\centering \textbf{Литература}}

% БИБЛИОГРАФИЯ

\begin{thebibliography}{0}
\bibitem{stud:b0}
Рекомендации по оформлению
и представлению курсовых
и выпускных квалификационных работ
студентов института математики,
механики и компьютерных наук.~--
Ростов н/Д, 2020.

\bibitem{stud:b1}
Жуков М.\,Ю., Ширяева Е.\,В.
\LaTeXe: искусство набора и вёрстки текстов с~формулами.~-- Ростов н/Д : Изд-во ЮФУ, 2009.

\bibitem{ALS:a1}{
	Gábor Takács, Domonkos Tikk,
	Alternating least squares for personalized ranking,
	DOI: 10.1145/2365952.2365972 
}

\bibitem{ALS:a2}{
	Github reporistory of "implicit" library,
	https://github.com/benfred/implicit,
	обр. 2021-12-28,
}

\bibitem{ALS:CFIFD}{
	Yifan Hu, Yehuda Koren, Chris Volinsky,
	Collaborative Filtering for Implicit Feedback Datasets,
	DOI: TODO
}

\bibitem{AE:a1}{
	Oleksii Kuchaiev, Boris Ginsburg,
	Training Deep AutoEncoders for Collaborative Filtering,
	https://arxiv.org/pdf/1708.01715.pdf 
}

\bibitem{DRN:a1}{
	Ali Elkahky, Yang Song, Xiaodong He,
	A Multi-View Deep Learning Approach for Cross Domain User Modeling in Recommendation Systems,
	https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/frp1159-songA.pdf 
}

\bibitem{INTRO:a1}
{
	https://www.mckinsey.com/industries/retail/our-insights/how-retailers-can-keep-up-with-consumers
}

\end{thebibliography}



\end{document}
% ----------------------------------------------------------------


\lstset{ %
language=Python,                 % выбор языка для подсветки (здесь это С++)
basicstyle=\small\sffamily, % размер и начертание шрифта для подсветки кода
numbers=left,               % где поставить нумерацию строк (слева\справа)
numberstyle=\tiny,           % размер шрифта для номеров строк
stepnumber=1,                   % размер шага между двумя номерами строк
numbersep=5pt,                % как далеко отстоят номера строк от подсвечиваемого кода
backgroundcolor=\color{white}, % цвет фона подсветки - используем \usepackage{color}
showspaces=false,            % показывать или нет пробелы специальными отступами
showstringspaces=false,      % показывать или нет пробелы в строках
showtabs=false,             % показывать или нет табуляцию в строках
frame=single,              % рисовать рамку вокруг кода
tabsize=2,                 % размер табуляции по умолчанию равен 2 пробелам
captionpos=t,              % позиция заголовка вверху [t] или внизу [b]
breaklines=true,           % автоматически переносить строки (да\нет)
breakatwhitespace=false, % переносить строки только если есть пробел
escapeinside={\%*}{*)}   % если нужно добавить комментарии в коде
extendedchars=true,
commentstyle=\color{mygreen},    % comment style
stringstyle=\bf,
commentstyle=\ttfamily\itshape,
keepspaces=true % пробелы между русскими буквами
aboveskip=3mm,
belowskip=3mm

}


\renewcommand\NAT@bibsetnum[1]{\settowidth\labelwidth{\@biblabel{#1}}%
   \setlength{\leftmargin}{\bibindent}\addtolength{\leftmargin}{\dimexpr\labelwidth+\labelsep\relax}%
   \setlength{\itemindent}{-\bibindent+\fivecharsapprox}%
   \setlength{\listparindent}{\itemindent}
\setlength{\itemsep}{\bibsep}\setlength{\parsep}{\z@}%
   \ifNAT@openbib
     \addtolength{\leftmargin}{\bibindent}%
     \setlength{\itemindent}{-\bibindent}%
     \setlength{\listparindent}{\itemindent}%
     \setlength{\parsep}{0pt}%
   \fi
}
\renewcommand{\thesection}{\arabic{section}.}
\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}.}
